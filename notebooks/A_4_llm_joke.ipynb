{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Joke (First LLM calls)\n",
    "\n",
    "This notebook demonstrates basic interactions with Language Models (LLMs) through:\n",
    "\n",
    "1. Configuring and selecting different LLM providers\n",
    "2. Sending prompts and receiving responses\n",
    "3. Dynamically switching between LLM models\n",
    "\n",
    "We'll use simple joke requests to test the LLM capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default Setup Cell.\n",
    "# It imports environment variables, define 'devtools.debug\" as a buildin, set PYTHONPATH and autorelaod\n",
    "# Copy it in other Notebooks\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Factory\n",
    "\n",
    "Our `LlmFactory` class provides a centralized way to:\n",
    "\n",
    "- Configure different LLM providers\n",
    "- Manage API keys and settings\n",
    "- Switch between models easily\n",
    "\n",
    "Let's see what models are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LlmInfo(id='parrot_local@fake', provider='fake', model='parrot', llm_args={}),\n",
       " LlmInfo(id='claude_opus46@openrouter', provider='openrouter', model='anthropic/claude-opus-4.6', llm_args={}),\n",
       " LlmInfo(id='claude_sonnet45@openrouter', provider='openrouter', model='anthropic/claude-sonnet-4.5', llm_args={}),\n",
       " LlmInfo(id='claude_haiku45@openrouter', provider='openrouter', model='anthropic/claude-haiku-4.5', llm_args={}),\n",
       " LlmInfo(id='gpt_52@openai', provider='openai', model='gpt-5.2', llm_args={}),\n",
       " LlmInfo(id='gpt_52@openrouter', provider='openrouter', model='openai/gpt-5.2', llm_args={}),\n",
       " LlmInfo(id='gpt_52mini@openai', provider='openai', model='gpt-5.2-mini', llm_args={}),\n",
       " LlmInfo(id='gpt_52mini@openrouter', provider='openrouter', model='openai/gpt-5.2-mini', llm_args={}),\n",
       " LlmInfo(id='gpt_52nano@openai', provider='openai', model='gpt-5.2-nano', llm_args={}),\n",
       " LlmInfo(id='gpt_51@openai', provider='openai', model='gpt-5.1', llm_args={}),\n",
       " LlmInfo(id='gpt_51@openrouter', provider='openrouter', model='openai/gpt-5.1', llm_args={}),\n",
       " LlmInfo(id='gpt_5@openai', provider='openai', model='gpt-5', llm_args={}),\n",
       " LlmInfo(id='gpt_5@openrouter', provider='openrouter', model='openai/gpt-5', llm_args={}),\n",
       " LlmInfo(id='gpt_41@openai', provider='openai', model='gpt-4.1', llm_args={}),\n",
       " LlmInfo(id='gpt_41@openrouter', provider='openrouter', model='openai/gpt-4.1', llm_args={}),\n",
       " LlmInfo(id='gpt_41@edenai', provider='edenai', model='gpt-4.1-2025-04-14', llm_args={}),\n",
       " LlmInfo(id='gpt_41@azure', provider='azure', model='gpt-4.1/2024-10-01-preview', llm_args={}),\n",
       " LlmInfo(id='gpt_41mini@openai', provider='openai', model='gpt-4.1-mini', llm_args={}),\n",
       " LlmInfo(id='gpt_41mini@openrouter', provider='openrouter', model='openai/gpt-4.1-mini', llm_args={}),\n",
       " LlmInfo(id='gpt_41mini@edenai', provider='edenai', model='gpt-4.1-mini-2025-04-14', llm_args={}),\n",
       " LlmInfo(id='gpt_4o@openai', provider='openai', model='gpt-4o', llm_args={}),\n",
       " LlmInfo(id='gpt_4o@edenai', provider='edenai', model='openai/gpt-4o', llm_args={}),\n",
       " LlmInfo(id='gpt_4o@azure', provider='azure', model='gpt-4o/2024-10-01-preview', llm_args={}),\n",
       " LlmInfo(id='gpt_4omini@openai', provider='openai', model='gpt-4o-mini', llm_args={}),\n",
       " LlmInfo(id='gpt_4omini@edenai', provider='edenai', model='openai/gpt-4o-mini', llm_args={}),\n",
       " LlmInfo(id='deepseek_v32@deepseek', provider='deepseek', model='deepseek-chat', llm_args={}),\n",
       " LlmInfo(id='deepseek_v32@openrouter', provider='openrouter', model='deepseek/deepseek-chat-v3.2', llm_args={}),\n",
       " LlmInfo(id='deepseek_v32@edenai', provider='edenai', model='deepseek/deepseek-chat', llm_args={}),\n",
       " LlmInfo(id='google_gemini3pro@google', provider='google', model='gemini-3-pro', llm_args={}),\n",
       " LlmInfo(id='google_gemini3pro@openrouter', provider='openrouter', model='google/gemini-3-pro', llm_args={}),\n",
       " LlmInfo(id='google_gemini3flash@google', provider='google', model='gemini-3-flash', llm_args={}),\n",
       " LlmInfo(id='google_gemini3flash@openrouter', provider='openrouter', model='google/gemini-3-flash', llm_args={}),\n",
       " LlmInfo(id='google_gemini25pro@google', provider='google', model='gemini-2.5-pro', llm_args={}),\n",
       " LlmInfo(id='google_gemini25pro@openrouter', provider='openrouter', model='google/gemini-2.5-pro', llm_args={}),\n",
       " LlmInfo(id='google_gemini25flashlite@google', provider='google', model='gemini-2.5-flash-lite', llm_args={}),\n",
       " LlmInfo(id='google_gemini25flashlite@openrouter', provider='openrouter', model='google/gemini-2.5-flash-lite', llm_args={}),\n",
       " LlmInfo(id='mistral_large3@mistralai', provider='mistralai', model='mistral-large-latest', llm_args={}),\n",
       " LlmInfo(id='mistral_large3@openrouter', provider='openrouter', model='mistralai/mistral-large-3', llm_args={}),\n",
       " LlmInfo(id='mistral_small32@mistralai', provider='mistralai', model='mistral-small-latest', llm_args={}),\n",
       " LlmInfo(id='mistral_small32@openrouter', provider='openrouter', model='mistralai/mistral-small-3.2', llm_args={}),\n",
       " LlmInfo(id='mistral_nemo@openrouter', provider='openrouter', model='mistralai/mistral-nemo', llm_args={}),\n",
       " LlmInfo(id='ministral_14b@mistralai', provider='mistralai', model='ministral-14b-latest', llm_args={}),\n",
       " LlmInfo(id='ministral_14b@openrouter', provider='openrouter', model='mistralai/ministral-8b-latest', llm_args={}),\n",
       " LlmInfo(id='ministral_8b@mistralai', provider='mistralai', model='ministral-8b-latest', llm_args={}),\n",
       " LlmInfo(id='llama4_maverick@groq', provider='groq', model='meta-llama/llama-4-maverick-17b-128e-instruct', llm_args={}),\n",
       " LlmInfo(id='llama4_maverick@openrouter', provider='openrouter', model='meta-llama/llama-4-maverick-17b-128e-instruct', llm_args={}),\n",
       " LlmInfo(id='llama33_70@groq', provider='groq', model='llama-3.3-70b-versatile', llm_args={}),\n",
       " LlmInfo(id='llama33_70@openrouter', provider='openrouter', model='meta-llama/llama-3.3-70b-instruct', llm_args={}),\n",
       " LlmInfo(id='llama33_70@edenai', provider='edenai', model='together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo', llm_args={}),\n",
       " LlmInfo(id='llama32_90V@deepinfra', provider='deepinfra', model='meta-llama/Llama-3.2-90B-Vision-Instruct', llm_args={}),\n",
       " LlmInfo(id='llama32_90V@openrouter', provider='openrouter', model='meta-llama/llama-3.2-90b-vision-instruct', llm_args={}),\n",
       " LlmInfo(id='qwen3_maxtinking@openrouter', provider='openrouter', model='qwen/qwen3-max-thinking', llm_args={}),\n",
       " LlmInfo(id='qwen3_coder_next@openrouter', provider='openrouter', model='qwen/qwen3-coder-next', llm_args={}),\n",
       " LlmInfo(id='qwen3_vl32@openrouter', provider='openrouter', model='qwen/qwen3-vl-32b-instruct', llm_args={}),\n",
       " LlmInfo(id='qwen25_coder32@openrouter', provider='openrouter', model='qwen/qwen-2.5-coder-32b-instruct', llm_args={}),\n",
       " LlmInfo(id='qwen2_vl72@openrouter', provider='openrouter', model='qwen/qwen-2-vl-72b-instruct', llm_args={}),\n",
       " LlmInfo(id='qwen_qwq32@deepinfra', provider='deepinfra', model='Qwen/QwQ-32B', llm_args={}),\n",
       " LlmInfo(id='qwen_qwq32@openrouter', provider='openrouter', model='qwen/qwq-32b', llm_args={}),\n",
       " LlmInfo(id='kimi_k25@openrouter', provider='openrouter', model='moonshotai/kimi-k2.5', llm_args={}),\n",
       " LlmInfo(id='kimi_k2@openrouter', provider='openrouter', model='moonshotai/kimi-k2-instruct', llm_args={}),\n",
       " LlmInfo(id='kimi_k2@groq', provider='groq', model='moonshotai/kimi-k2-instruct', llm_args={}),\n",
       " LlmInfo(id='nova_pro@openrouter', provider='openrouter', model='amazon/nova-pro-v1', llm_args={}),\n",
       " LlmInfo(id='nova_lite@openrouter', provider='openrouter', model='amazon/nova-lite-v1', llm_args={}),\n",
       " LlmInfo(id='minimax_m25@openrouter', provider='openrouter', model='minimax/minimax-m2.5', llm_args={}),\n",
       " LlmInfo(id='gpt_oss120@openrouter', provider='openrouter', model='openai/gpt-oss-120b', llm_args={}),\n",
       " LlmInfo(id='gpt_oss120@groq', provider='groq', model='openai/gpt-oss-120b', llm_args={}),\n",
       " LlmInfo(id='gpt_oss20@openrouter', provider='openrouter', model='openai/gpt-oss-20b', llm_args={}),\n",
       " LlmInfo(id='gpt_oss20@groq', provider='groq', model='openai/gpt-oss-20b', llm_args={}),\n",
       " LlmInfo(id='sonar_pro@openrouter', provider='openrouter', model='perplexity/sonar-pro', llm_args={}),\n",
       " LlmInfo(id='sonar_reasoning@openrouter', provider='openrouter', model='perplexity/sonar-reasoning', llm_args={}),\n",
       " LlmInfo(id='liquid_lfm40@openrouter', provider='openrouter', model='liquid/lfm-40b', llm_args={}),\n",
       " LlmInfo(id='nvidia_nemotron70@deepinfra', provider='deepinfra', model='nvidia/Llama-3.1-Nemotron-70B-Instruct', llm_args={}),\n",
       " LlmInfo(id='nvidia_nemotron70@openrouter', provider='openrouter', model='nvidia/llama-3.1-nemotron-70b-instruct', llm_args={}),\n",
       " LlmInfo(id='glm_5@openrouter', provider='openrouter', model='z-ai/glm-5', llm_args={}),\n",
       " LlmInfo(id='falcon3_3@ollama', provider='ollama', model='falcon3', llm_args={}),\n",
       " LlmInfo(id='gemma3_4b@ollama', provider='ollama', model='gemma3:4b', llm_args={}),\n",
       " LlmInfo(id='llava_phi3@ollama', provider='ollama', model='llava-phi3', llm_args={}),\n",
       " LlmInfo(id='mistral_7b@openrouter', provider='openrouter', model='mistralai/mistral-7b-instruct:free', llm_args={}),\n",
       " LlmInfo(id='mistral_7b@custom', provider='custom', model='mistralai/Mistral-7B-Instruct-v0.3', llm_args={'base_url': 'https://gateway.cp4sc.platform.codex-platform.com/mistral/v1', 'default_headers': {'X-Gravitee-Api-Key': '${oc.env:GRAVITEE_KEY, null }'}}),\n",
       " LlmInfo(id='zephyr_7b@huggingface', provider='huggingface', model='HuggingFaceH4/zephyr-7b-beta', llm_args={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from genai_tk.core.llm_factory import LlmFactory, get_llm\n",
    "\n",
    "LlmFactory.known_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an LLM\n",
    "\n",
    "We can either:\n",
    "1. Get the default LLM with basic configuration\n",
    "2. Select a specific model by ID\n",
    "\n",
    "The factory handles all the provider-specific setup for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-16 16:13:03.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgenai_tk.core.llm_factory\u001b[0m:\u001b[36mget_llm\u001b[0m:\u001b[36m688\u001b[0m - \u001b[34m\u001b[1mget LLM:'gpt_41@azure' -extra: {'temperature': 0.5}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AzureChatOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4.1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">profile</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'max_input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1047576</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'max_output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32768</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_inputs'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'audio_inputs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'video_inputs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_outputs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'audio_outputs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'video_outputs'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_output'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'tool_calling'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'structured_output'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_url_inputs'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pdf_inputs'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pdf_tool_message'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'image_tool_message'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'tool_choice'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">client</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">openai.resources.chat.completions.completions.Completions</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7968ac8c9220</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7968ac15f200</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">root_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.lib.azure.AzureOpenAI object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7968ac61f740</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">root_async_client</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;openai.lib.azure.AsyncAzureOpenAI object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7968ac8adbb0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4.1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">stream_usage</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">max_retries</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">disabled_params</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'parallel_tool_calls'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">azure_endpoint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://mutualizedopenai.openai.azure.com'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">deployment_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4.1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_version</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2024-10-01-preview'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">openai_api_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'azure'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAzureChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mname\u001b[0m=\u001b[32m'gpt-4.1'\u001b[0m,\n",
       "    \u001b[33mprofile\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'max_input_tokens'\u001b[0m: \u001b[1;36m1047576\u001b[0m,\n",
       "        \u001b[32m'max_output_tokens'\u001b[0m: \u001b[1;36m32768\u001b[0m,\n",
       "        \u001b[32m'image_inputs'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'audio_inputs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'video_inputs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'image_outputs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'audio_outputs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'video_outputs'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'reasoning_output'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'tool_calling'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'structured_output'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'image_url_inputs'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'pdf_inputs'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'pdf_tool_message'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'image_tool_message'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'tool_choice'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7968ac8c9220\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x7968ac15f200\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_client\u001b[0m\u001b[39m=<openai.lib.azure.AzureOpenAI object at \u001b[0m\u001b[1;36m0x7968ac61f740\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_async_client\u001b[0m\u001b[39m=<openai.lib.azure.AsyncAzureOpenAI object at \u001b[0m\u001b[1;36m0x7968ac8adbb0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mmodel_name\u001b[0m=\u001b[32m'gpt-4.1'\u001b[0m,\n",
       "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5\u001b[0m,\n",
       "    \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mstream_usage\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mmax_retries\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "    \u001b[33mseed\u001b[0m=\u001b[1;36m42\u001b[0m,\n",
       "    \u001b[33mdisabled_params\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'parallel_tool_calls'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mazure_endpoint\u001b[0m=\u001b[32m'https://mutualizedopenai.openai.azure.com'\u001b[0m,\n",
       "    \u001b[33mdeployment_name\u001b[0m=\u001b[32m'gpt-4.1'\u001b[0m,\n",
       "    \u001b[33mopenai_api_version\u001b[0m=\u001b[32m'2024-10-01-preview'\u001b[0m,\n",
       "    \u001b[33mopenai_api_type\u001b[0m=\u001b[32m'azure'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the default LLM. We can configure the temperature, cache, max_token, ...\n",
    "\n",
    "\n",
    "llm_default = get_llm(temperature=0.5)\n",
    "print(llm_default)\n",
    "\n",
    "# or get a given LLM;\n",
    "gpt4o = LlmFactory(llm=\"gpt_4o@edenai\").get()  # Might NOT work if you din't have the API key\n",
    "\n",
    "# or select by tag;\n",
    "fake = LlmFactory(llm=\"fake\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Prompts\n",
    "\n",
    "Now let's send our first prompt to the LLM.\n",
    "\n",
    "We'll use the `invoke()` method which:\n",
    "- Takes a string prompt\n",
    "- Returns the model's response\n",
    "- Handles all the API communication\n",
    "\n",
    "We'll ask for a joke to test the model's creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field.\\n\\nBut nobody was </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there to clap for him.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'token_usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4.1-2025-04-14'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fp_7a7fd0eb44'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-D9uZTrgFYGHYKIdqTZSKmdndjbd5K'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_filter_results'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'content_filter_results'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'hate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'jailbreak'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'detected'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'self_harm'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'sexual'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'violence'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'content_filter_results'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'hate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'protected_material_code'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'detected'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'protected_material_text'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'detected'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'self_harm'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'sexual'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'violence'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filtered'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'severity'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'safe'</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'lc_run--019c6703-1e97-7ac1-bf53-154521bf4ceb-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">invalid_tool_calls</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_token_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'audio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field.\\n\\nBut nobody was \u001b[0m\n",
       "\u001b[32mthere to clap for him.'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "            \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m39\u001b[0m,\n",
       "            \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'model_provider'\u001b[0m: \u001b[32m'openai'\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-4.1-2025-04-14'\u001b[0m,\n",
       "        \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_7a7fd0eb44'\u001b[0m,\n",
       "        \u001b[32m'id'\u001b[0m: \u001b[32m'chatcmpl-D9uZTrgFYGHYKIdqTZSKmdndjbd5K'\u001b[0m,\n",
       "        \u001b[32m'prompt_filter_results'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'prompt_index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'content_filter_results'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'hate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'jailbreak'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'detected'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'self_harm'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'sexual'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'violence'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[32m'content_filter_results'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'hate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'protected_material_code'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'detected'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'protected_material_text'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'detected'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'self_harm'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'sexual'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'violence'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filtered'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'severity'\u001b[0m: \u001b[32m'safe'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'lc_run--019c6703-1e97-7ac1-bf53-154521bf4ceb-0'\u001b[0m,\n",
       "    \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minvalid_tool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m39\u001b[0m,\n",
       "        \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cache_read'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'reasoning'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joke1 = llm_default.invoke(\"tell me a sad joke\")\n",
    "print(joke1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-bp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
